<div style="background-color: rgba(255, 0, 0, 0.1); color: red; padding: 12px; text-align: center; font-weight: bold; border: 2px solid red; border-radius: 6px; font-size: 18px;">
  âš ï¸âš ï¸ <span style="color: black;">ğŸ”§ Currently Being Updated</span> âš ï¸âš ï¸
</div>


# ğŸ–ï¸ Hand Detection with Landmarks

A PyTorch-based deep learning project focused on detecting multiple hands in an image and predicting precise landmark keypoints for each hand. This repository serves as a foundational module for gesture recognition, sign language translation, and human-computer interaction systems.

---

## ğŸš€ Features

- ğŸ” **Multi-hand detection** capability
- ğŸ¯ **21 keypoint landmark localization** per detected hand
- ğŸ“¦ **Modular dataset loader**, currently tailored for the [FreiHAND dataset](https://lmb.informatik.uni-freiburg.de/projects/freihand/).
- ğŸ’¾ **Validation & checkpointing** integrated
- ğŸ† **Supports augmentation handling and custom training splits**
- ğŸ”¥ Designed for **real-time extensions and downstream tasks**

---

## ğŸ—‚ï¸ Project Structure
```
.
â””â”€â”€ hand-detection-landmarks/
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ params
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ freihanddataset.py                  # Custom Dataset class for FreiHAND
    â”‚   â””â”€â”€ preprocess.py                       # Conversion of 3D keypoints to 2D
    â”œâ”€â”€ dataset                                 # FreiHAND dataset (not included)
    â”œâ”€â”€ inference_images                        # Output folder for inference images
    â”œâ”€â”€ outputs/
    â”‚   â”œâ”€â”€ checkpoints                         # Saved checkpoints (not included)
    â”‚   â””â”€â”€ plots                               # Saved plots from training (not included)
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ model/
    â”‚   â”‚   â””â”€â”€ landmarkmodel.py                # Model architecture definition
    â”‚   â”œâ”€â”€ utils/
    â”‚   â”‚   â”œâ”€â”€ accuracy/
    â”‚   â”‚   â”‚   â””â”€â”€ custom_accuracies.py        # IoU and PCB for accuracy metrics
    â”‚   â”‚   â””â”€â”€ losses/
    â”‚   â”‚       â”œâ”€â”€ custom_loss_functions.py    # SmoothL1 and WingLoss
    â”‚   â”‚       â””â”€â”€ wing_loss.py                # WingLoss class    
    â”‚   â”œâ”€â”€ inference.py
    â”‚   â””â”€â”€ train.py
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ README.md
    â””â”€â”€ requirements.txt
```

---

## ğŸ Quick Start

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/uzayyildiztaskan/Hand-Detection-Landmarks
cd Hand-Detection-Landmarks
```
### 2ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```
### 3ï¸âƒ£ Prepare Dataset
Download the FreiHAND dataset

Place files in:

```
dataset/FreiHAND_pub_v2/
```
### 4ï¸âƒ£ Train the Model
```
python -m src.train
```
### 5ï¸âƒ£ Checkpoints & Validation
Checkpoints are saved in ```/checkpoints/``` when validation loss improves. Configure validation split in ```config/params.py```.

### 5ï¸âƒ£ Inference
After specifying ```IMAGE_PATH``` and ```CHECKPOINT_PATH``` in ```inference.py``` run the script by
```
python -m src.inference
```

![Sample image inference from dataset](https://github.com/uzayyildiztaskan/Hand-Detection-Landmarks/blob/main/inference_images/Figure_1.png)

Infered model parameters:

* Epochs: ```15```
* Layers Unfreezed: ```6```
* Train Loss: ```10.9017```, Val Loss: ```14.3914```
* Train IoU: ```0.8973```, Val IoU: ```0.8919```
* Train PCK: ```0.9942```, Val PCK: ```0.9826```
* ALPHA: ```1.0```
* BETA: ```1.0```
* Margin of bounding box: ```0```

---

## ğŸ“Œ Future Work

* Sign language translation pipeline integration

* Real-time inference demo (OpenCV/Streamlit)

---

## ğŸ“œ License
MIT License - see [LICENSE](https://github.com/uzayyildiztaskan/Hand-Detection-Landmarks/blob/main/LICENSE) for details.
